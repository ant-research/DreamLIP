# DreamLIP: Language-Image Pre-training with Long Captions
Official repo for [DreamLIP: Language-Image Pre-training with Long Captions]().


![timeline.jpg](figures/moti.png)

> **DreamLIP: Language-Image Pre-training with Long Captions** <br>

[//]: # (> [Kecheng Zheng]&#40;https://zkcys001.github.io/&#41;, [Yujun Shen]&#40;https://shenyujun.github.io/&#41; <br>)
[//]: # (> **xxx Track** <br>)

[[Paper](https://arxiv.org/pdf/2403.17007.pdf)]

## Overview of supported long captions:

<details open>
<summary><b>Long Captions of Supported Datasets (5)</b></summary>

> - [x] [![](https://img.shields.io/badge/CC3M-f4d5b3?style=for-the-badge)](https://ai.google.com/research/ConceptualCaptions/)
> - [x] [![](https://img.shields.io/badge/CC12M-d0e9ff?style=for-the-badge)](https://github.com/google-research-datasets/conceptual-12m)
> - [x] [![](https://img.shields.io/badge/YFCC15M-yellowgreen?style=for-the-badge)](https://github.com/Sense-GVT/DeCLIP/blob/main/docs/dataset_prepare.md)
> - [ ] [![](https://img.shields.io/badge/Laion-c2e2de?style=for-the-badge)](https://laion.ai/laion-5b-a-new-era-of-open-large-scale-multi-modal-datasets/)
> - [ ] [![](https://img.shields.io/badge/Coyo-854?style=for-the-badge)](https://github.com/kakaobrain/coyo-dataset)
</details>
<details open>
<summary><b>Long Captions of MLLMs (3)</b></summary>

> - [x] ![](https://img.shields.io/badge/InstructBLIP-f4d5b3?style=for-the-badge) 
> - [x] ![](https://img.shields.io/badge/LLAVA1.5-d0e9ff?style=for-the-badge) 
> - [x] ![](https://img.shields.io/badge/SHAREGPT4V-854?style=for-the-badge) 

</details>

## TODO

- [ ] Release code.
- [ ] Release weights.
- [ ] Release long caption extracted from LLAVA1.5, InstructBLIP and shareGPT4V.

[//]: # (## Acknowledgement)

## BibTeX

```bibtex
@article{DreamLIP,
  title={DreamLIP: Language-Image Pre-training with Long Captions},
  author={Zheng, Kecheng and Zhang, Yifei and Wu, Wei and Lu, Fan and Ma, Shuailei and Jin, Xin and Chen, Wei and Shen, Yujun},
  journal={arXiv:2403.17007},
  year={2024}
}
```

